{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_website(url):\n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all divs with the class \"obj_issue_summary\"\n",
    "        issue_summaries = soup.find_all('div', class_='obj_issue_summary')\n",
    "        \n",
    "        return issue_summaries\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the website. Status code: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_article_summaries(url):\n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all divs with the class \"obj_article_summary\"\n",
    "        article_summaries = soup.find_all('div', class_='obj_article_summary')\n",
    "        \n",
    "        return article_summaries\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the website. Status code: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(issue_summaries):\n",
    "    urls = []\n",
    "    for issue_summary in issue_summaries:\n",
    "        # Find the a class title\n",
    "        link = issue_summary.find('a', class_='title', href=True)\n",
    "\n",
    "        title_text = link.text\n",
    "        \n",
    "        # Check if AAAI-24 in title\n",
    "        if 'AAAI-24' not in link.text:\n",
    "            continue\n",
    "        # Append the URL to the list of URLs\n",
    "        urls.append((title_text, link['href']))\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ojs.aaai.org/index.php/AAAI/issue/archive'  # Replace with the target website URL\n",
    "summaries = crawl_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = get_urls(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url 1\n",
      "url 2\n",
      "url 3\n",
      "url 4\n",
      "url 5\n",
      "url 6\n",
      "url 7\n",
      "url 8\n",
      "url 9\n",
      "url 10\n",
      "url 11\n",
      "url 12\n",
      "url 13\n",
      "url 14\n",
      "url 15\n",
      "url 16\n",
      "url 17\n",
      "url 18\n",
      "url 19\n",
      "url 20\n",
      "url 21\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "issue_summaries = []\n",
    "for url in urls:\n",
    "    issue_summaries.append(crawl_article_summaries(url[1]))\n",
    "    print('url', len(issue_summaries))\n",
    "    time.sleep(2)  # Sleep for 1 second to avoid spamming the website\n",
    "\n",
    "# Flatten the list of issue summaries\n",
    "issue_summaries = [summary for summaries in issue_summaries for summary in summaries]\n",
    "print(len(issue_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1014/2865 [18:41:30<6838:50:55, 13300.84s/it]"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tqdm\n",
    "\n",
    "def clear_string(input_str):\n",
    "    # Use regular expression to remove backslashes and the following character\n",
    "    cleaned_str = re.sub(r'\\\\.', '', input_str)\n",
    "    return cleaned_str.strip()\n",
    "\n",
    "data = []\n",
    "with tqdm.tqdm(total=len(issue_summaries)) as pbar:\n",
    "    for issue_summary in issue_summaries:\n",
    "        current = {}\n",
    "        # Find the h3 tag\n",
    "        h3 = issue_summary.find('h3')\n",
    "        title = clear_string(h3.text)\n",
    "\n",
    "        current['title'] = title\n",
    "        current['authors'] = []\n",
    "        \n",
    "        # Find the a tag\n",
    "        href = h3.find('a')['href']\n",
    "\n",
    "        for i in range(5):\n",
    "            try:\n",
    "                response = requests.get(href)\n",
    "\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                authors = soup.find('ul', class_='authors')\n",
    "                authors_list = authors.find_all('li')\n",
    "\n",
    "                for author in authors_list:\n",
    "                    cur_author = {}\n",
    "                    cur_author['name'] = clear_string(author.find('span', class_='name').text)\n",
    "                    cur_author['affiliation'] = clear_string(author.find('span', class_='affiliation').text)\n",
    "                    current['authors'].append(cur_author)\n",
    "\n",
    "                data.append(current)\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(2)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the data to a JSON file\n",
    "import json\n",
    "\n",
    "with open('data/aaai24.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
